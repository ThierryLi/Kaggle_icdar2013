{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/icdar2013-gender-prediction-from-handwriting<br> https://www.pyimagesearch.com/2017/12/11/image-classification-with-keras-and-deep-learning/<br> \n",
    "https://medium.com/@kylepob61392/airplane-image-classification-using-a-keras-cnn-22be506fdb53<br> \n",
    "http://overfitt.io/posts/large-image-datasets-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\Python\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing the necessary package\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "import glob\n",
    "import os.path as path\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the train answer.csv file\n",
    "train_answers = pd.read_csv('train_answers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating the path of each images and sorting it\n",
    "image_path = 'data'\n",
    "file_paths = glob.glob(path.join(image_path,'*.jpg'))\n",
    "file_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a new dataframe with only the file_path and the writer ID\n",
    "COLUMN_NAMES=['file_path', 'writer']\n",
    "dataframe = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "dataframe['file_path']= file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From the dataframe, we need to extract the writer ID from the file path\n",
    "# the format of the file path is 'data\\<writer_id>_<number>.jpg\n",
    "dataframe['writer'] = dataframe['file_path'].str.extract(r'(\\d+)_(\\d+).jpg', expand=True)\n",
    "# Set the writer column as a Category\n",
    "dataframe['writer'] = pd.Categorical(dataframe['writer']).codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we need to merge the dataframe to the train_answer dataframe in order to add the male\n",
    "# column for each writer\n",
    "merged_dataframe = pd.merge(dataframe, train_answers, how ='right', on = ['writer'])\n",
    "# as in the train_answer we only have identified 282 writers, every writer IDs > 282 will be\n",
    "# removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we can build the X_train and y_train dataframe to be used by the model\n",
    "X = merged_dataframe['file_path']\n",
    "y = merged_dataframe['male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>writer</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>data\\0034_4.jpg</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           file_path  writer  male\n",
       "131  data\\0034_4.jpg      33     0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading a specific line and checking if the write type is correct\n",
    "test_data = merged_dataframe.loc[merged_dataframe['file_path'] == 'data\\\\0034_4.jpg']\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Keras image generators allow you to preprocess batches of images in real-time.\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=True, # set input mean to 0 over the dataset\n",
    "        samplewise_center=False, # set each sample mean to 0\n",
    "        featurewise_std_normalization=True, # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False, # divide each input by its std\n",
    "        zca_whitening=False, # apply ZCA whitening\n",
    "        rotation_range=20, # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2, # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2, # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False, # randomly flip images\n",
    "        vertical_flip=False) # randomly flip images\n",
    "\n",
    "\n",
    "# Hyperparameter: defining how many convolutional layers for our CNN model\n",
    "N_layers = 4\n",
    "\n",
    "def cnn(size, n_layers):\n",
    "    # size : size of the input images\n",
    "    # n_layers : number of layers\n",
    "    # model : compiled CNN\n",
    "    \n",
    "    # define local hyperparameters\n",
    "    min_neurons = 20\n",
    "    max_neurons = 120\n",
    "    kernel = (3,3)\n",
    "    \n",
    "    # determine the number of neurons in each convolutionnal layer\n",
    "    steps = np.floor(max_neurons / (n_layers + 1))\n",
    "    neurons = np.arange(min_neurons, max_neurons, steps)\n",
    "    neurons = neurons.astype(np.int32)\n",
    "    \n",
    "    # define a model \n",
    "    model = Sequential()\n",
    "\n",
    "    # add convolutional layers\n",
    "    for  i in range(0,n_layers):\n",
    "        if i == 0:\n",
    "            shape = (size[0],size[1], size[2])\n",
    "            model.add(Conv2D(neurons[i], kernel, input_shape= shape))\n",
    "        else:\n",
    "            model.add(Conv2D(neurons[i], kernel))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        #Add max pooling layer\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(max_neurons))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        #Add output layer\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        \n",
    "        #complie the model\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        # print a summary of the model\n",
    "        model.summary()\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.preprocessing import image\n",
    "#test_image = image.load_img(X_train[1], \n",
    "#            target_size = (244, 244))\n",
    "#test_image = image.img_to_array(test_image)\n",
    "#\n",
    "## scale the raw pixel intensities to the range [0, 1]\n",
    "#test_image = np.array(test_image, dtype=\"float\") / 255.0\n",
    "##test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 242, 242, 20)      560       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 242, 242, 20)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 121, 121, 20)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 292820)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               35138520  \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 121       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 35,139,201\n",
      "Trainable params: 35,139,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiating the model\n",
    "image_size = (244,244,3)\n",
    "model = cnn(size = image_size , n_layers = N_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining functions to read and convert the images into numpy array\n",
    "def load_my_image(img_path, target_size=(244, 244)):\n",
    "#    print('Image path size: ')\n",
    "#    display(type(img_path))\n",
    "#    im = load_img(img_path, target_size=target_size)\n",
    "    images = [misc.imread(path, target_size=target_size) for path in file_paths]\n",
    "#    images = [misc.imread(path) for path in  X_batches[0]]    \n",
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "    images = np.array(images, dtype=\"float\") / 255.0\n",
    "#  return img_to_array(im) #converts image to numpy array\n",
    "    return images #converts image to numpy array\n",
    "             \n",
    "def split_and_load(X_samples, y_samples, batch_size=100):\n",
    "    target_size=(244, 244)\n",
    "    batch_size = len(X_samples) / batch_size\n",
    "    X_batches = np.array_split(X_samples, batch_size)\n",
    "    y_batches = np.array_split(y_samples, batch_size)\n",
    "#    display('After the splitting')\n",
    "#    display('before the map call')\n",
    "#    display(type( X_batches[0]))\n",
    "#    test = np.array(map(load_my_image, X_batches[0]))\n",
    "#    test = X_batches[0].map(load_my_image)\n",
    "#    display('after the map call')\n",
    "    \n",
    "#    display(test.shape)\n",
    "    for b in range(len(X_batches)):\n",
    "#        display(b)\n",
    "        print('Before loading the image X_batches: ', b , len(X_batches[b]))\n",
    "        print('Before loading the image y_batches: ', b , len(y_batches[b]))\n",
    "#        x = np.array(map(load_my_image, X_batches[b]))\n",
    "        x = [img_to_array(load_img(path, target_size=target_size)) for path in X_batches[b]]\n",
    "        x = np.array(x, dtype=\"float\") / 255.0\n",
    "        y = np.array(y_batches[b])\n",
    "        display('After loading the image x: ', x.size)\n",
    "        display('After loading the image y: ', y.size)        \n",
    "        yield x, y             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Before loading the image X_batches:  0 103\n",
      "Before loading the image y_batches:  0 103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'After loading the image x: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "18396624"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'After loading the image y: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\Python\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:536: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn'tbeen fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Temp\\Python\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:544: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn'tbeen fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 1\n",
    "batch_size = 32\n",
    "for e in range(n_epoch):\n",
    "  print( \"epoch\", e)\n",
    "  for X_train, y_train in split_and_load(X, y):# chunks of 100 images\n",
    "#        display(X_train.size)\n",
    "#        display(y_train.size)\n",
    "        for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=32):# chunks of 32 samples\n",
    "            loss = model.train_on_batch(X_batch, y_batch, verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a prediction on the test set\n",
    "test_predictions = model.predict(x_test)\n",
    "test_predictions = np.round(test_predictions)\n",
    "# Report the accuracy\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(\"Accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_incorrect_labels(x_data, y_real, y_predicted):\n",
    "    # INPUTS\n",
    "    # x_data      - images\n",
    "    # y_data      - ground truth labels\n",
    "    # y_predicted - predicted label\n",
    "    count = 0\n",
    "    figure = plt.figure()\n",
    "    incorrect_label_indices = (y_real != y_predicted)\n",
    "    y_real = y_real[incorrect_label_indices]\n",
    "    y_predicted = y_predicted[incorrect_label_indices]\n",
    "    x_data = x_data[incorrect_label_indices, :, :, :]\n",
    "\n",
    "    maximum_square = np.ceil(np.sqrt(x_data.shape[0]))\n",
    "\n",
    "    for i in range(x_data.shape[0]):\n",
    "        count += 1\n",
    "        figure.add_subplot(maximum_square, maximum_square, count)\n",
    "        plt.imshow(x_data[i, :, :, :])\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Predicted: \" + str(int(y_predicted[i])) + \", Real: \" + str(int(y_real[i])), fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_incorrect_labels(x_test, y_test, np.asarray(test_predictions).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the full batch of images\n",
    "all_images = [img_to_array(load_img(path, target_size=(64,64))) for path in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_array = np.array(all_images, dtype=\"float\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1128, 64, 64, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1128"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1128, 12288)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_val1, y_train1, y_val1 = train_test_split(images_array, y, test_size = 0.22, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "879"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(879, 12288)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scikit-learn expects 2d num arrays for the training dataset for a fit function. The dataset \n",
    "# you are passing in is a 3d array you need to reshape the array into a 2d.\n",
    "dataset_size = len(x_train1)\n",
    "display(dataset_size)\n",
    "TwoDim_dataset = x_train1.reshape(dataset_size,-1)\n",
    "display(TwoDim_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nec9L3WAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2654e9483c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pca = PCA().fit(TwoDim_dataset)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "#plt.xlim([-10,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
